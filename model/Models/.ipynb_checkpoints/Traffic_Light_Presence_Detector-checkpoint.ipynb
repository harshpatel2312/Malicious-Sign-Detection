{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe065ced-8514-4c5e-b9a2-b2ccf89f5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from screeninfo import get_monitors\n",
    "import pickle\n",
    "import numpy as np\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgba2rgb, gray2rgb\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import threading\n",
    "import logging\n",
    "from flask import Flask, jsonify\n",
    "import time\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1527d943-5216-4846-982d-e35affa7f29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image_with_unknown(img, model, threshold=0.4):\n",
    "   \n",
    "    #img = imread(image_path)\n",
    "    ##RGB conversion\n",
    "    if len(img.shape) == 2:\n",
    "        img = gray2rgb(img)\n",
    "    elif img.shape[-1] == 4:\n",
    "        img = rgba2rgb(img)\n",
    "\n",
    "    #Prediction\n",
    "    img = resize(img, img_size, anti_aliasing=True).flatten()\n",
    "    probabilities = model.predict_proba(img.reshape(1, -1))\n",
    "    max_confidence = np.max(probabilities)\n",
    "    \n",
    "    return model.predict(img.reshape(1, -1))[0] if max_confidence >= threshold else \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0269060-20da-4c85-babf-0e67e908a2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(image, test_results, best_estimator):\n",
    "    if image is not None and image.size  > 0:\n",
    "        # Classify the image\n",
    "        result = classify_image_with_unknown(image, best_estimator, threshold=0.4)\n",
    "        test_results[f\"frame_{frame_count}\"] = result\n",
    "        print(f\"Frame: {frame_count}, Classification Result: {result}\")\n",
    "    else:\n",
    "        print(f\"Skipping frame_{frame_count}: Not a useful frame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2372ae28-97c7-430f-b8f0-44e614d7fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad18658e-acc6-4ae1-9974-66542f403914",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_flask():\n",
    "    app.run(debug=False, port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df3e4adc-4f73-4271-951e-b29f2551fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/data', methods=['GET'])\n",
    "def get_data():\n",
    "    return jsonify(metrics), 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7247158a-862b-418e-b831-281c27ed86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "threading.Thread(target=run_flask, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46db1906-68f7-4e2b-94e3-bbcafb5347be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "C:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.3.2 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Loading the trained model\n",
    "model_file_name = r\"E:\\Education\\Projects\\Machine Learning\\Computer Vision\\Malicious-Sign-Detection\\model\\Models\\Trained_with_threshold.pkl\"\n",
    "with open(model_file_name, 'rb') as file:\n",
    "    best_estimator = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e488ca44-19e7-4e63-94af-17b258a86e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading calssification_report\n",
    "with open('Scripts/Resources/classification_report', 'rb') as f:\n",
    "    report = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef036e0-ca22-474e-802d-b4ce7490455f",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_macro = report['macro avg']['precision']\n",
    "recall_macro = report['macro avg']['recall']\n",
    "f1_score_macro = report['macro avg']['f1-score']\n",
    "support_macro = report['macro avg']['support']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4b8011-ec6d-445e-9354-9b7258561e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    #\"Classification_Result\": result,\n",
    "    #\"Validation_Accuracy\": accuracy,\n",
    "    \"Execution_Time_of_Prediction\": None,\n",
    "    \"Macro_Precision\": precision_macro,\n",
    "    \"Macro_Recall\": recall_macro,\n",
    "    \"Macro_F1_score\": f1_score_macro,\n",
    "    \"Macro_Support\": support_macro,\n",
    "    \"Test_Results\" : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6485936f-7c0c-4721-a1c4-a4d3f331ff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (20, 20) # Resizing for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1148d0a3-d01d-49b0-8872-ff0279d9d64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get screen resolution\n",
    "screen = get_monitors()[0]  # Get the primary monitor\n",
    "screen_width, screen_height = screen.width, screen.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bdd5b68-bbeb-467c-a422-d3dc44621097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model for traffic light detection\n",
    "model = YOLO('yolov8n.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "214ced11-dceb-47e2-872b-a938c6a94a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the video\n",
    "video_path = r\"..\\Resources\\Videos\\Real_life_test_night_3.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e2037f1-dd27-44f1-84ca-42c34dc3d3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video file!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5040e4e7-f2f9-4fef-8e33-e354ab6e161c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 car, 93.2ms\n",
      "Speed: 3.2ms preprocess, 93.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 66.2ms\n",
      "Speed: 2.6ms preprocess, 66.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 79.5ms\n",
      "Speed: 2.9ms preprocess, 79.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 87, Classification Result: green\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 65.8ms\n",
      "Speed: 3.0ms preprocess, 65.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 116, Classification Result: green\n",
      "\n",
      "0: 384x640 2 cars, 1 traffic light, 1 fire hydrant, 59.9ms\n",
      "Speed: 2.5ms preprocess, 59.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 145, Classification Result: green\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 1 fire hydrant, 83.1ms\n",
      "Speed: 2.7ms preprocess, 83.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 174, Classification Result: green\n",
      "\n",
      "0: 384x640 2 cars, 60.7ms\n",
      "Speed: 3.0ms preprocess, 60.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 1 traffic light, 65.3ms\n",
      "Speed: 2.4ms preprocess, 65.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 232, Classification Result: red\n",
      "\n",
      "0: 384x640 2 cars, 61.7ms\n",
      "Speed: 2.1ms preprocess, 61.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 cars, 69.6ms\n",
      "Speed: 3.1ms preprocess, 69.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 52.3ms\n",
      "Speed: 2.4ms preprocess, 52.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 62.6ms\n",
      "Speed: 2.3ms preprocess, 62.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 cars, 58.5ms\n",
      "Speed: 2.4ms preprocess, 58.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 55.7ms\n",
      "Speed: 2.2ms preprocess, 55.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 63.8ms\n",
      "Speed: 2.2ms preprocess, 63.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 cars, 61.6ms\n",
      "Speed: 2.3ms preprocess, 61.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 55.7ms\n",
      "Speed: 2.1ms preprocess, 55.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 59.0ms\n",
      "Speed: 2.5ms preprocess, 59.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 57.0ms\n",
      "Speed: 2.1ms preprocess, 57.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 umbrella, 60.0ms\n",
      "Speed: 2.2ms preprocess, 60.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 55.4ms\n",
      "Speed: 2.9ms preprocess, 55.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 traffic lights, 60.1ms\n",
      "Speed: 2.2ms preprocess, 60.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 638, Classification Result: red\n",
      "\n",
      "0: 384x640 (no detections), 57.6ms\n",
      "Speed: 2.1ms preprocess, 57.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 55.7ms\n",
      "Speed: 2.1ms preprocess, 55.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 56.2ms\n",
      "Speed: 2.5ms preprocess, 56.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 56.8ms\n",
      "Speed: 2.3ms preprocess, 56.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 2 cars, 53.8ms\n",
      "Speed: 2.2ms preprocess, 53.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 53.5ms\n",
      "Speed: 2.3ms preprocess, 53.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 60.6ms\n",
      "Speed: 2.2ms preprocess, 60.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 78.5ms\n",
      "Speed: 2.3ms preprocess, 78.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 59.2ms\n",
      "Speed: 2.1ms preprocess, 59.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 56.8ms\n",
      "Speed: 2.6ms preprocess, 56.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 traffic light, 58.5ms\n",
      "Speed: 2.1ms preprocess, 58.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 957, Classification Result: yellow\n",
      "\n",
      "0: 384x640 1 traffic light, 55.3ms\n",
      "Speed: 2.2ms preprocess, 55.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 986, Classification Result: yellow\n",
      "\n",
      "0: 384x640 3 cars, 1 fire hydrant, 58.0ms\n",
      "Speed: 2.2ms preprocess, 58.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 60.3ms\n",
      "Speed: 2.1ms preprocess, 60.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1044, Classification Result: red\n",
      "\n",
      "0: 384x640 1 car, 57.5ms\n",
      "Speed: 2.3ms preprocess, 57.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 52.1ms\n",
      "Speed: 2.6ms preprocess, 52.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 59.9ms\n",
      "Speed: 2.2ms preprocess, 59.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 69.9ms\n",
      "Speed: 2.8ms preprocess, 69.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 60.7ms\n",
      "Speed: 2.4ms preprocess, 60.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 56.7ms\n",
      "Speed: 2.6ms preprocess, 56.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 car, 61.9ms\n",
      "Speed: 2.2ms preprocess, 61.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 car, 58.9ms\n",
      "Speed: 2.3ms preprocess, 58.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 cars, 81.1ms\n",
      "Speed: 2.2ms preprocess, 81.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 56.3ms\n",
      "Speed: 2.3ms preprocess, 56.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 57.7ms\n",
      "Speed: 2.3ms preprocess, 57.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 56.3ms\n",
      "Speed: 2.4ms preprocess, 56.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 53.4ms\n",
      "Speed: 2.5ms preprocess, 53.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1421, Classification Result: red\n",
      "\n",
      "0: 384x640 1 car, 2 traffic lights, 52.2ms\n",
      "Speed: 2.2ms preprocess, 52.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1450, Classification Result: red\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 59.7ms\n",
      "Speed: 2.6ms preprocess, 59.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1479, Classification Result: red\n",
      "\n",
      "0: 384x640 1 car, 55.6ms\n",
      "Speed: 2.2ms preprocess, 55.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 58.2ms\n",
      "Speed: 2.6ms preprocess, 58.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 traffic light, 55.0ms\n",
      "Speed: 2.4ms preprocess, 55.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1566, Classification Result: red\n",
      "\n",
      "0: 384x640 2 traffic lights, 59.4ms\n",
      "Speed: 2.3ms preprocess, 59.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1595, Classification Result: red\n",
      "\n",
      "0: 384x640 1 traffic light, 53.6ms\n",
      "Speed: 2.0ms preprocess, 53.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1624, Classification Result: green\n",
      "\n",
      "0: 384x640 1 car, 2 traffic lights, 57.0ms\n",
      "Speed: 2.3ms preprocess, 57.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1653, Classification Result: red\n",
      "\n",
      "0: 384x640 2 traffic lights, 55.7ms\n",
      "Speed: 2.5ms preprocess, 55.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1682, Classification Result: yellow\n",
      "\n",
      "0: 384x640 1 car, 1 traffic light, 57.9ms\n",
      "Speed: 2.5ms preprocess, 57.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1711, Classification Result: red\n",
      "\n",
      "0: 384x640 1 car, 2 traffic lights, 64.3ms\n",
      "Speed: 2.5ms preprocess, 64.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1740, Classification Result: red\n",
      "\n",
      "0: 384x640 2 traffic lights, 58.6ms\n",
      "Speed: 2.2ms preprocess, 58.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1769, Classification Result: red\n",
      "\n",
      "0: 384x640 2 traffic lights, 60.6ms\n",
      "Speed: 2.2ms preprocess, 60.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1798, Classification Result: red\n",
      "\n",
      "0: 384x640 1 person, 61.0ms\n",
      "Speed: 2.7ms preprocess, 61.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 traffic light, 54.3ms\n",
      "Speed: 2.3ms preprocess, 54.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1856, Classification Result: red\n",
      "\n",
      "0: 384x640 1 person, 1 traffic light, 58.8ms\n",
      "Speed: 2.3ms preprocess, 58.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1885, Classification Result: red\n",
      "\n",
      "0: 384x640 2 traffic lights, 61.3ms\n",
      "Speed: 2.5ms preprocess, 61.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1914, Classification Result: red\n",
      "\n",
      "0: 384x640 1 traffic light, 70.2ms\n",
      "Speed: 2.5ms preprocess, 70.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1943, Classification Result: red\n",
      "\n",
      "0: 384x640 1 traffic light, 58.0ms\n",
      "Speed: 2.5ms preprocess, 58.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 1972, Classification Result: red\n",
      "\n",
      "0: 384x640 1 traffic light, 53.7ms\n",
      "Speed: 2.3ms preprocess, 53.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame: 2001, Classification Result: red\n",
      "Total frames processed: 2001\n"
     ]
    }
   ],
   "source": [
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_count = 0\n",
    "\n",
    "cv2.namedWindow(\"Video\", cv2.WINDOW_NORMAL)  # Create a resizable window\n",
    "start_time = time.time()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"End of video or cannot access the video.\")\n",
    "        break\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    frame_count += 1\n",
    "    # Process every frame or skip to reduce processing\n",
    "    if frame_count % int(fps) != 0:  # Process one frame per second\n",
    "        continue\n",
    "\n",
    "    # YOLO detection\n",
    "    results = model(frame)  # Run the YOLO model on the current frame\n",
    "    \n",
    "    # Access the first result in the list\n",
    "    result = results[0]\n",
    "    \n",
    "    test_results = {}\n",
    "    \n",
    "    # Iterate through detected boxes\n",
    "    for box in result.boxes:\n",
    "        class_id = int(box.cls)  # Object class ID\n",
    "        if class_id == 9:  # class 9 corresponds to traffic lights in YOLO\n",
    "            # Extract bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Bounding box coordinates\n",
    "            traffic_light_roi = frame[y1:y2, x1:x2]  # Crop the traffic light region\n",
    "\n",
    "            test(traffic_light_roi, test_results, best_estimator)\n",
    "            metrics[\"Test_Results\"].append(test_results)\n",
    "\n",
    "            break  # Avoid multiple saves for the same frame\n",
    "            \n",
    "    # Resize frame to fit screen size\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "    aspect_ratio = frame_width / frame_height\n",
    "\n",
    "    # Calculate new dimensions while maintaining aspect ratio\n",
    "    if frame_width > screen_width or frame_height > screen_height:\n",
    "        if frame_width / screen_width > frame_height / screen_height:\n",
    "            new_width = screen_width\n",
    "            new_height = int(screen_width / aspect_ratio)\n",
    "        else:\n",
    "            new_height = screen_height\n",
    "            new_width = int(screen_height * aspect_ratio)\n",
    "    else:\n",
    "        new_width, new_height = frame_width, frame_height\n",
    "\n",
    "    resized_frame = cv2.resize(frame, (new_width, new_height))\n",
    "    cv2.imshow(\"Video\", resized_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "metrics[\"Execution_Time_of_Prediction\"] = execution_time\n",
    "\n",
    "print(f\"Total frames processed: {frame_count}\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
